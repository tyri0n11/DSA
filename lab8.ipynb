{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyri0n11/DSA/blob/main/lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I. Theoretical session:\n",
        "1. Could you list out some limitation of MapReduce?\n",
        "2. Provide a high level comparison of Apache Hadoop and Apache Spark.\n",
        "3. What are the advantages of Apache Spark?\n",
        "4. Provide a comparison of RDD and DataFrame in Spark.  "
      ],
      "metadata": {
        "id": "V-S6ggQMmxvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "II. You are given a file `appl_stock.csv`, please carry out the following tasks:\n",
        "\n",
        "1. Read this file by PySpark. Print out the schema.\n",
        "2. Create new columns of combining the High, Low, Close and Adj Close as follow `[High, Low, Close, Adj Close]`.\n",
        "3. Create a new column which computes the average price of High and Low prices.\n",
        "4. Create a new column which computes the amount of money based on the formula `Volume * Adj Close`.\n",
        "3. Using `groupby` and `year()` function to compute the average closing price per year.\n"
      ],
      "metadata": {
        "id": "S9_563ulpsh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "III. You are given a data `customer_churn.csv`, which describes the churn status in clients of a marletting agency. As a data scientist, you are required to create a machine learning model **in Spark** that will help predict which customers will churn (stop buying their service). A short description of the data is as follow:\n",
        "```\n",
        "Name : Name of the latest contact at Company\n",
        "Age: Customer Age\n",
        "Total_Purchase: Total Ads Purchased\n",
        "Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
        "Years: Totaly Years as a customer\n",
        "Num_sites: Number of websites that use the service.\n",
        "Onboard_date: Date that the name of the latest contact was onboarded\n",
        "Location: Client HQ Address\n",
        "Company: Name of Client Company\n",
        "```\n",
        "\n",
        "1. Read, print the schema and check out the data to set the first sight of the data.\n",
        "2. Format the data according to `VectorAssembler`, which is supported in MLlib of PySpark.\n",
        "3. Split the data into train/test data, and then fit train data to the logistic regression model.\n",
        "4. Evaluate the results and compute the AUC."
      ],
      "metadata": {
        "id": "brQ8gRaUshXy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fN4Zb88PtzCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}